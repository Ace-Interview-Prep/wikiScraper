{-# LANGUAGE LambdaCase #-}
{-# LANGUAGE DeriveAnyClass #-}
{-# LANGUAGE DeriveGeneric  #-}
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE TupleSections #-}


module Concurrent where

import Types
import StartGenre (startGenre)
import ScrapeWiki

import Scrappy.Requests
import Scrappy.Scrape
-- import Scrappy.Elem.ChainHTML
-- import Scrappy.Elem.SimpleElemParser
-- import Scrappy.Elem.Types (getHrefEl, innerHtmlFull, Elem', ElemHead, attrs, elTag, noPat)
import Scrappy.Elem
import Scrappy.Links
import Scrappy.BuildActions (catEithers) -- TODO: Move to Scrappy.Types

import GHC.Conc (atomically)
import Control.Concurrent (forkIO, setNumCapabilities)
import Control.Concurrent.STM.TVar (TVar, newTVarIO, writeTVar, readTVar)
import Control.Monad.IO.Class (liftIO)
import Control.Monad.Trans.State (StateT, modify, runStateT, gets)
import Data.Functor (void)
import Database.PostgreSQL.Simple 
import Network.HTTP.Client (newManager, Manager)
import Network.HTTP.Client.TLS (tlsManagerSettings)
import Text.Parsec (anyChar, (<|>), parse, many, try, digit)
import Control.Applicative (liftA2, some)
import Data.Maybe (catMaybes, isJust, fromJust, fromMaybe)
import Data.Text (Text, pack, unpack, strip)
import Data.Text.Encoding (encodeUtf8)
import Data.Traversable (for)

import Data.Time.Clock (getCurrentTime, diffUTCTime)
import qualified Data.Set as Set
import qualified Data.Map as Map
import qualified Data.Map.Strict as StrictMap
import Data.Tree
import Data.Either (fromRight)
import qualified Data.Aeson as Aeson
import GHC.Generics
import System.Directory
import Crypto.Hash (SHA256(SHA256))
import Crypto.MAC (hmacAlg, hmacGetDigest)
import Data.Bifunctor (bimap)
import Data.ByteString.Lazy (toStrict)
import Text.CSV

import Data.UUID (toString)
import Data.UUID.V4 (nextRandom)

import Control.Monad (forever, when)
import System.Time.Extra (sleep)

  
baseResultsFolder = "/home/lazylambda/code/Ace/wikiResults/"
baseWorkingFolder = "working/"
--baseResultsFolder = "/home/lazylambda/code/Ace/wikiResults/" 

main :: IO ()
main = do
  mgr <- newManager tlsManagerSettings
  conn <- connectPostgreSQL "postgresql://lazylambda:Warhawks58!@localhost/wikiscraper"
  -- set capabilities to max
  putStrLn "NOTE: in order to get this done quickly, this must complete initialization all together / not partially. Init is done when you see \"genres initialized\""
  sleep 5
  putStrLn "How many processors would you like to use?"
  
  numProcessors <- (\s -> parse (read <$> (some digit)) "" s) <$> getLine
  putStrLn $ "Using " <> (show $ fromRight 4 $ numProcessors) <> " processors"
  genres <- parseGenres <$> (readFile "genres.csv" )

  
  
  genresRef <- newTVarIO genres
  setNumCapabilities $ fromRight 4 $ numProcessors
  print "start genres"

  -- This will always be the last one to get initialized 
  hasBeenInitialized <- doesDirectoryExist $ baseWorkingFolder <> "Public and International Affairs/0"
  print hasBeenInitialized
  when (not hasBeenInitialized) $ startWriteOriginalRefIns conn mgr genres 
  print "genres initialized"
  -- | This will do both the initialization and then writing one starting Link as a RefIn
  -- | To wikiResults/<genre>/0/start.json
  
  mapM_ (\_ -> forkIO $ procMain conn mgr genresRef) [1..(fromRight 4 $ numProcessors)] 


procMain :: Connection -> Manager -> TVar [(Importance, Genre)] -> IO ()
procMain conn mgr genresRef = forever $ do
  print "start procMain"
  genre <- getNextGenre genresRef 
  processGenre conn mgr genre
  print "got here"
  putGenreBack genresRef genre 
  print "finished putGenreBack"
  
getNextGenre :: TVar [(Importance, Genre)] -> IO (Importance, Genre)
getNextGenre genresRef = atomically $ do
  genres <- readTVar genresRef
  writeTVar genresRef (tail genres)
  pure (head genres)

processGenre :: Connection -> Manager -> (Importance, Genre) -> IO ()
processGenre conn mgr (importance, genre) = do
  putStrLn $ "For " <> (unpack genre) <> " I will run " <> (show importance) <> " refIns"
  refIns <- for [1..importance] $ \_ -> takeRefIn genre

  

  print $ length refIns
  print "got hereeeeeee"
  mapM_ (processChunk conn mgr) $ catMaybes refIns

processChunk :: Connection -> Manager -> STM_RefIn -> IO ()
processChunk conn mgr (STM_RefIn refIn links) = do
  
  mapM_ (runWikiLink conn mgr refIn) links 


-- TODO(galen): make this a ReaderT
runWikiLink :: Connection -> Manager -> RefIn -> Link -> IO ()   
runWikiLink conn mgr refIn link = do
  print "got here"
  writeRefInToDB conn refIn link 
  
  liftIO $ print link
--  liftIO $ print (depthPrev + 1) 
  (_, html) <- liftIO $ getHtml mgr link
  uuid' <- nextRandom 
  let
    depth = getDepth refIn 
    genre = getGenre refIn
    uuid = toString uuid'
    filepath = baseWorkingFolder <> (unpack genre) <> "/" <> (show depth) <> "/" <> uuid <> ".json" 
    (links, wikiPage) = scrapeWikiPage (genre, depth) link html

  startEvalPage <- liftIO getCurrentTime
  print "write page"
  liftIO $ writePage genre depth link wikiPage
  print "done write page"
  endEvalPage <- liftIO getCurrentTime
  liftIO $ print (diffUTCTime endEvalPage startEvalPage)

  let nextDepth = depth + 1 

  writeSTM_RefIn genre nextDepth (Transient genre nextDepth link) links 
  print $ "wrote STM_RefIn" <> (show nextDepth) <> " " <> (unpack genre)


writeRefInToDB :: Connection -> RefIn -> Link -> IO ()
writeRefInToDB conn refIn link =
  void $ execute conn "insert into reference_ins(link, link_from) values(?,?)" $ (show link, show refIn)

putGenreBack :: TVar [(Importance, Genre)] -> (Importance, Genre) -> IO ()
putGenreBack genresRef genre = atomically $ do
  genres <- readTVar genresRef
  writeTVar genresRef $ genres <> [genre] 

startWriteOriginalRefIns :: Connection -> Manager -> [(Importance, Genre)] -> IO ()
startWriteOriginalRefIns conn mgr genres = do
  linksWithGenre <- mapM (\(_,g) -> (g,) <$> (startGenre mgr $ unpack g)) genres
  mapM_ (\(g, link) ->  write g 0 conn link (Start g)) linksWithGenre
        
  where
    write :: Genre -> Depth -> Connection -> Link -> RefIn -> IO () 
    write genre depth conn link refIn = do
      writeSTM_RefIn genre depth refIn [link]
      void $ execute conn "insert into reference_ins(link, link_from) values(?,?)" $ (show link, show refIn)
      

writeSTM_RefIn :: Genre -> Depth -> RefIn -> [Link] -> IO ()
writeSTM_RefIn genre depth refIn links = do
  uuid <- toString <$> nextRandom 
  let dir = baseWorkingFolder <> (unpack genre) <> "/" <> (show depth) <> "/"
  let path = dir <> uuid <> ".json" 
  createDirectoryIfMissing True dir 
  Aeson.encodeFile path $ STM_RefIn refIn links
  
  
parseGenres :: String -> [(Importance, Genre)]
parseGenres s = fmap toTup $ fromRight undefined $ parseCSV "" s 
  where
--    toTup [] = []
    toTup (x:y:[]) = (read x, pack y)
    

-- | Destructively take a file ie remove it
-- | This assumes a folder model of the following
-- | We continually create <something>/folder_n+1/
-- | And consume the min of n's children by 'taking' (read then delete) 
-- | In the special case that we consume more than has been created, then this will return Nothing 
takeRefIn :: Genre -> IO (Maybe STM_RefIn)
takeRefIn genre' = takeRefIn' genre' 0 
  where
    takeRefIn' :: Genre -> Depth -> IO (Maybe STM_RefIn)
    takeRefIn' genre n = do
      let folderPath = baseWorkingFolder <> (unpack genre) <> "/" <> (show n) <> "/"
      doesDirectoryExist folderPath >>= \case
        False -> pure Nothing 
        True -> do
          -- A folder could exist and already be used up 
          files <- listDirectory folderPath
          case null files of
            True -> takeRefIn' genre (n + 1)
            False -> do
              refIn <- Aeson.decodeFileStrict $ folderPath <> (head files) -- safe cuz definitely not null
              print refIn
              removeFile $ folderPath <> (head files) 
              pure refIn 
  

--processRefIn :: STM_RefIn -> IO () 


  -- this is kinda bracket 
